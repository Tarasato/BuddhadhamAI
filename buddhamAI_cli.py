# buddhamAI_cli.py
import sys
import os
import pickle
import json
import time
import subprocess
import traceback
import numpy as np
import faiss
import ollama
import hashlib
from debugger import conn_str
from sqlalchemy import create_engine, text
from reDocuments import ensure_embeddings_up_to_date
from debugger import format_duration, log

try:
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stderr.reconfigure(encoding="utf-8")
    
    def clear_screen():
        if os.name == 'nt':  # Windows
            os.system('cls')
        else:  # Unix/Linux/Mac
            os.system('clear')
    
    log_file = "buddhamAI_cli.log"
    required_models = ["gpt-oss:20b", "nomic-embed-text:v1.5"]
    EMB_PATH = "embeddings.npy"
    META_PATH = "metadata.pkl"
    DOCS_ALL_PATH = "documents/documentsPkl/documents_all.pkl"
    start = None
    end = None
    STATUS_FILE = "embed_status.json"
    debug_mode = os.getenv("DEBUG", "false").lower()
    engine = create_engine(conn_str, fast_executemany=True)

    def get_installed_models():
        # try --json
        try:
            result = subprocess.run(
            ["ollama", "list", "--json"],
            capture_output=True,
            text=True
            )
            if result.returncode == 0 and result.stdout.strip().startswith("["):
                return [m["name"] for m in json.loads(result.stdout)]
        except Exception:
            pass
        # fallback to parsing text output
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True
        )
        lines = result.stdout.strip().split("\n")
        models = []
        for line in lines[1:]:  # skip header
            parts = line.split()
            if parts:
                models.append(parts[0])
        return models

    def check_and_pull_models(models_to_check):
        try:
            local_model_names = get_installed_models()
            missing_models = [m for m in models_to_check if m not in local_model_names]

            if missing_models:
                for model in missing_models:
                    log(f"üì• Installing {model} ...")
                    subprocess.run(["ollama", "pull", model], check=True)
                    log(f"‚úÖ Finished installing {model}")
            else:
                log("‚úÖ All models are present")
        except Exception:
            log("‚ùå Error:\n" + traceback.format_exc())
            exit(1)

    def flatten_docs(raw):
        docs = []
        for book_name, chapters in raw.items():
            for chapter_key, pages in chapters.items():
                for page_key, content in pages.items():
                    chapter_num = int(chapter_key.replace("chapter ", ""))
                    docs.append({
                        "bookname": book_name,
                        "chapter": chapter_num,
                        "content": content
                    })
        return docs

    def load_embeddings_and_metadata():
        log(f"use embeddings {required_models[1]}")
        log(f"Loading {EMB_PATH} and {META_PATH}")
        embeddings = np.load(EMB_PATH)
        with open(META_PATH, 'rb') as f:
            metadata = pickle.load(f)
        return embeddings, metadata

    def search(query, index, metadata, top_k, max_distance):
        log(f"Searching for references for: {query} with top_k={top_k} and max_distance={max_distance}")

        # Create embedding for query
        q_emb = np.array([ollama.embeddings(model='nomic-embed-text:v1.5', prompt=query)['embedding']], dtype='float32')

        # Fetch more from FAISS ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ filter max_distance
        fetch_k = top_k * 5
        distances, ids = index.search(q_emb, fetch_k)
        log(f"Found {len(ids[0])} nearest neighbors (fetched {fetch_k})")

        results = []
        filtered_out_results = []
        seen_docs = set()

        for i, idx in enumerate(ids[0]):
            if idx >= len(metadata):
                continue

            dist = distances[0][i]
            doc = metadata[idx]
            doc_hash = hashlib.md5(doc['content'].encode('utf-8')).hexdigest()

            # filter duplicates ‡πÅ‡∏•‡∏∞ max_distance
            if doc_hash in seen_docs or (max_distance is not None and dist > max_distance):
                filtered_out_results.append((doc, dist, idx))
                log(f"index={idx}, distance={dist:.4f} removed")
                continue

            results.append({
                "doc": doc,
                "distance": dist,
                "index": idx
            })
            seen_docs.add(doc_hash)
            log(f"index={idx}, distance={dist:.4f} added")

            # stop ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ top_k
            if len(results) >= top_k:
                break

        log(f"Searching for references found {len(results)} items: {short_references([r['doc'] for r in results])}")

        if filtered_out_results:
            contexts = [f"{doc['content']}" for doc, _, _ in filtered_out_results]
            full_context = "\n".join(contexts)
            log(f"Filtered out references {len(filtered_out_results)} items: {short_references([doc for doc, _, _ in filtered_out_results])}")
            log(f"Filtered out contexts:\n{full_context}")

        return results

        
    def short_references(metadata):
        sorted_docs = sorted(metadata, key=lambda d: (d['bookName'], d['chapterName']))
        return ", ".join([
            f"{d['bookName']} - {d['chapterName']}"
            for d in sorted_docs
        ])
        
    def is_about_buddhism_db(text_to_check) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° text ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÉ‡∏ô chapter_tb ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        join ‡∏Å‡∏±‡∏ö book_tb ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠
        return: True ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á, False ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        with engine.connect() as conn:
            query = text("""
            SELECT COUNT(*) AS cnt
            FROM chapter_tb c
            INNER JOIN book_tb b ON c.bookId = b.bookId
            WHERE c.chapterText LIKE :text OR b.bookName LIKE :text
            """)
            result = conn.execute(query, {"text": f"%{text_to_check}%"})
            count = result.scalar()
            return count > 0
        
    def filter_buddhism_response(response_text) -> str:
        if is_about_buddhism_db(response_text):
            return response_text
        else:
            return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    def check_rejection_message(text: str) -> bool:
        rejection_phrases = [
            "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°",
            "‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ",
            "‡∏Ñ‡∏∏‡∏ì‡∏ä‡πà‡∏ß‡∏¢‡∏ä‡∏µ‡πâ‡πÅ‡∏à‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°",
            "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏à‡πâ‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô",
            "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö",
            "‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£",
            "‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        ]
        return any(phrase in text for phrase in rejection_phrases)
    
    def check_greeting_message(text: str) -> bool:
        greeting_phrases = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö",
        ]
        return any(phrase in text for phrase in greeting_phrases)

    def ask(query, index, metadata, top_k=None, max_distance=None):
        global start
        start = time.perf_counter()
        top_k = len(query) if top_k is None else top_k
        if top_k > 10:
            top_k = 10
        log(f"Asking question: {query}")
        if is_about_buddhism_db(query) == False:
            end = time.perf_counter()
            processing_time = format_duration(end - start)
            return {
                "answer": "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô",
                "duration": f'‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {processing_time}'
            }
        else: 
            results = search(query, index, metadata, top_k, max_distance=max_distance)

        contexts = [r["doc"]["content"] for r in results]
        
        full_context = "\n".join(contexts)
        prompt = f"""‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{full_context}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}"""
        model = 'gpt-oss:20b'
        log(f"Asking model: \"{model}\" with prompt:\n{prompt}")
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏£‡∏£‡∏°‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö...‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ....‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡∏°‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"},
                {"role": "user", "content": prompt}
            ]
        )
        raw_answer = response['message']['content']
        answer = filter_buddhism_response(raw_answer)
        ref_text = short_references([r["doc"] for r in results])
        end = time.perf_counter()
        processing_time = format_duration(end - start)
        log(f"Asked \"{model}\" finished in {processing_time}")
        if check_rejection_message(answer) or check_greeting_message(answer):
            return {
                "answer": answer,
                "duration": f'‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {processing_time}'
            }
        else:
            return {
                "answer": answer,
                "references": f"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {ref_text}",
                "duration": f'‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {processing_time}'
            }
    
    def read_last_embed_time():
        if not os.path.exists(STATUS_FILE):
            return "1970-01-01 00:00:00"
        with open(STATUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)["last_embed_time"]

    def init_bot():
        log("Checking for data updates...")
        embeddings, metadata = ensure_embeddings_up_to_date()
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        return index, metadata

    def parse_args(argv):
        message = None
        top_k = None
        max_distance = None

        i = 0
        while i < len(argv):
            arg = argv[i]
            if arg == '-k' and i + 1 < len(argv):
                try:
                    top_k = int(argv[i+1])
                except ValueError:
                    top_k = None
                i += 2
            elif arg == '-d' and i + 1 < len(argv):
                try:
                    max_distance = float(argv[i+1])
                except ValueError:
                    max_distance = None
                i += 2
            elif not arg.startswith('-') and message is None:
                message = arg
                i += 1
            else:
                i += 1

        return message, top_k, max_distance


    def ask_cli(argv=None):
        try:
            log("Starting BuddhamAI")
            if debug_mode == "false":
                check_and_pull_models(required_models)

            if argv is None:  # if not provided ‚Üí use sys.argv
                argv = sys.argv[1:]

            message, top_k, max_distance = parse_args(argv)

            if message is None or message.strip() == "":
                    result = {"answer": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", "references": "‡πÑ‡∏°‡πà‡∏°‡∏µ", "duration": format_duration(0)}
                    data = {"data": result}
                    json_str = json.dumps(data, ensure_ascii=False)
                    log(json_str)
                    print(json_str)
                    return data  # return to main.py

            index, metadata = init_bot()
            if debug_mode == "true":
                time.sleep(float(os.getenv("DEBUG_TIME")))
                result = {"answer": "**‡∏ò‡∏£‡∏£‡∏° (Dhamma) ‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤‡∏™‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡πå**  - **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°**: ‚Äú‡∏ò‡∏£‡∏£‡∏°‚Äù ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‡∏Å‡∏é‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ò‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏£‡∏∞‡∏û‡∏∏‡∏ó‡∏ò‡πÄ‡∏à‡πâ‡∏≤‡∏ú‡∏π‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πå  - **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö‡∏≠‡∏£‡∏¥‡∏¢‡∏™‡∏±‡∏à 4**:    1. **‡∏ó‡∏∏‡∏Å‡∏Ç‡πå** ‚Äì ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï    2. **‡∏™‡∏°‡∏∏‡∏ó‡∏±‡∏¢** ‚Äì ‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ç‡πå (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡∏ì‡∏´‡∏≤, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡∏≤‡∏Å)    3. **‡∏ô‡∏¥‡πÇ‡∏£‡∏ò** ‚Äì ‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πå (‡∏™‡∏±‡∏ô‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πå)    4. **‡∏°‡∏£‡∏£‡∏Ñ** ‚Äì ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á (‡∏≠‡∏£‡∏¥‡∏¢‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πå  - **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ï‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°**:    * ‡∏ù‡∏∂‡∏Å‡∏™‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô    * ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏°‡∏£‡∏£‡∏Ñ (‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á 8 ‡∏™‡πà‡∏ß‡∏ô)    * ‡∏•‡∏î‡∏ï‡∏±‡∏ì‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏ß‡∏á‡∏´‡∏≤‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ò‡∏£‡∏£‡∏°  ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ ‚Äú‡∏ò‡∏£‡∏£‡∏°‚Äù ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏é‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πå ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏≠‡∏£‡∏¥‡∏¢‡∏™‡∏±‡∏à 4 ‡πÅ‡∏•‡∏∞‡∏°‡∏£‡∏£‡∏Ñ.", "argv": message, "references": "test only", "duration": format_duration(0)} # for test only
                data = {"data": result}
                json_str = json.dumps(data, ensure_ascii=False)
                log(json_str)
                print(json_str)
                return data  # return to main.py
            
            result = ask(message, index, metadata, top_k=top_k, max_distance=max_distance)

            data = {"data": result}
            json_str = json.dumps(data, ensure_ascii=False)
            log(json_str)
            print(json_str)
            return data  # return to main.py
        except Exception:
            err_msg = traceback.format_exc()
            log("Error: " + err_msg)
            result = {"Error": f"Error: {err_msg}", "status": 500}
            data = {"data": result}
            json_str = json.dumps(data, ensure_ascii=False)
            log(json_str)
            print(json_str)
            return data  # return to main.py

    if __name__ == "__main__":
        ask_cli()
        
except Exception:
    err_msg = traceback.format_exc()
    try:
        log("Error: " + err_msg)
        result = {"Error": f"Error: {err_msg}", "status": 500}
        data = {"data": result}
        json_str = json.dumps(data, ensure_ascii=False)
        log(json_str)
        print(json_str)
    except:
        log("Error: " + err_msg)
        result = {"Error": f"Error: {err_msg}", "status": 500}
        data = {"data": result}
        json_str = json.dumps(data, ensure_ascii=False)
        log(json_str)
        print(json_str)
    sys.exit(1)