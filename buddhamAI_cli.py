# buddhamAI_cli.py
import sys
import os
import pickle
import json
import time
import subprocess
import traceback
import numpy as np
import faiss
import ollama
import hashlib
from numpy.linalg import norm
from sklearn.metrics.pairwise import cosine_similarity
from reDocuments import ensure_embeddings_up_to_date
from debugger import format_duration, log

try:
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stderr.reconfigure(encoding="utf-8")
    
    def clear_screen():
        if os.name == 'nt':  # Windows
            os.system('cls')
        else:  # Unix/Linux/Mac
            os.system('clear')
    
    log_file = "buddhamAI_cli.log"
    required_models = ["gpt-oss:20b", "nomic-embed-text:v1.5"]
    EMB_PATH = "embeddings.npy"
    META_PATH = "metadata.pkl"
    DOCS_ALL_PATH = "documents/documentsPkl/documents_all.pkl"
    start = None
    end = None
    STATUS_FILE = "embed_status.json"
    debug_mode = os.getenv("DEBUG", "false").lower()
    
    with open(log_file, "w", encoding="utf-8") as f:
        f.write("")

    def get_installed_models():
        # try --json
        try:
            result = subprocess.run(
            ["ollama", "list", "--json"],
            capture_output=True,
            text=True
            )
            if result.returncode == 0 and result.stdout.strip().startswith("["):
                return [m["name"] for m in json.loads(result.stdout)]
        except Exception:
            pass
        # fallback to parsing text output
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True
        )
        lines = result.stdout.strip().split("\n")
        models = []
        for line in lines[1:]:  # skip header
            parts = line.split()
            if parts:
                models.append(parts[0])
        return models

    def check_and_pull_models(models_to_check):
        try:
            local_model_names = get_installed_models()
            missing_models = [m for m in models_to_check if m not in local_model_names]

            if missing_models:
                for model in missing_models:
                    log(f"ðŸ“¥ Installing {model} ...")
                    subprocess.run(["ollama", "pull", model], check=True)
                    log(f"âœ… Finished installing {model}")
            else:
                log("âœ… All models are present")
        except Exception:
            log("âŒ Error:\n" + traceback.format_exc())
            exit(1)

    def flatten_docs(raw):
        docs = []
        for book_name, chapters in raw.items():
            for chapter_key, pages in chapters.items():
                for page_key, content in pages.items():
                    chapter_num = int(chapter_key.replace("chapter ", ""))
                    docs.append({
                        "bookname": book_name,
                        "chapter": chapter_num,
                        "content": content
                    })
        return docs

    def load_embeddings_and_metadata():
        log(f"use embeddings {required_models[1]}")
        log(f"Loading {EMB_PATH} and {META_PATH}")
        embeddings = np.load(EMB_PATH)
        with open(META_PATH, 'rb') as f:
            metadata = pickle.load(f)
        return embeddings, metadata

    def search(query, index, metadata, top_k, max_distance):
        log(f"Searching for references for: {query} with top_k={top_k} and max_distance={max_distance}")

        # Create embedding for query
        q_emb = np.array([ollama.embeddings(model='nomic-embed-text:v1.5', prompt=query)['embedding']], dtype='float32')

        # Fetch more from FAISS à¹€à¸œà¸·à¹ˆà¸­ filter max_distance
        fetch_k = top_k * 5
        distances, ids = index.search(q_emb, fetch_k)
        log(f"Found {len(ids[0])} nearest neighbors (fetched {fetch_k})")

        results = []
        filtered_out_results = []
        seen_docs = set()

        for i, idx in enumerate(ids[0]):
            if idx >= len(metadata):
                continue

            dist = distances[0][i]
            doc = metadata[idx]
            doc_hash = hashlib.md5(doc['content'].encode('utf-8')).hexdigest()

            # filter duplicates à¹à¸¥à¸° max_distance
            if doc_hash in seen_docs or (max_distance is not None and dist > max_distance):
                filtered_out_results.append((doc, dist, idx))
                log(f"index={idx}, distance={dist:.4f} removed")
                continue

            results.append({
                "doc": doc,
                "distance": dist,
                "index": idx
            })
            seen_docs.add(doc_hash)
            log(f"index={idx}, distance={dist:.4f} added")

            # stop à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸”à¹‰ top_k
            if len(results) >= top_k:
                break

        log(f"Searching for references found {len(results)} items: {short_references([r['doc'] for r in results])}")

        if filtered_out_results:
            contexts = [f"{doc['content']}" for doc, _, _ in filtered_out_results]
            full_context = "\n".join(contexts)
            log(f"Filtered out references {len(filtered_out_results)} items: {short_references([doc for doc, _, _ in filtered_out_results])}")
            log(f"Filtered out contexts:\n{full_context}")

        return results

        
    def short_references(metadata):
        sorted_docs = sorted(metadata, key=lambda d: (d['bookName'], d['chapterName']))
        return ", ".join([
            f"{d['bookName']} - {d['chapterName']}"
            for d in sorted_docs
        ])

    def filter_user_query(query: str):
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² EMB_PATH à¹‚à¸«à¸¥à¸”à¹„à¸”à¹‰à¹„à¸«à¸¡
        try:
            emb_matrix = np.load(EMB_PATH)
        except Exception as e:
            log(f"Error loading EMB_PATH: {EMB_PATH}, exception: {e}")
            return None

        log(f"Loaded emb_matrix shape: {emb_matrix.shape}")
        
        # à¸”à¸¶à¸‡ embedding à¸‚à¸­à¸‡ query
        query_emb = np.array(
            ollama.embeddings(model="nomic-embed-text:v1.5", prompt=query)["embedding"]
        )
        log(f"query_emb shape: {query_emb.shape}")

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š norm à¸‚à¸­à¸‡ query
        query_norm = np.linalg.norm(query_emb)
        log(f"query_emb norm: {query_norm:.6f}")
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š norm à¸‚à¸­à¸‡à¸—à¸¸à¸ embedding
        emb_norms = np.linalg.norm(emb_matrix, axis=1)
        zero_indices = np.where(emb_norms == 0)[0]
        if query_norm == 0:
            log("Warning: query embedding is zero vector!")
        if len(zero_indices) > 0:
            log(f"Warning: found zero vector(s) in embeddings at indices {zero_indices}")
        
        # à¸„à¸³à¸™à¸§à¸“ cosine similarity à¸”à¹‰à¸§à¸¢ sklearn
        similarities = cosine_similarity(query_emb.reshape(1, -1), emb_matrix)[0]
        
        top_index = np.argmax(similarities)
        log(f"à¸„à¹‰à¸™à¸«à¸²à¸”à¹‰à¸§à¸¢ '{query}' à¹€à¸ˆà¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸à¸¥à¹‰à¸ªà¸¸à¸”à¸—à¸µà¹ˆ index {top_index}, similarity = {similarities[top_index]:.6f}")
        
        return similarities[top_index]

    def check_rejection_message(text: str) -> bool:
        rejection_phrases = [
            "à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡",
            "à¹„à¸¡à¹ˆà¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸‚à¸­à¸­à¸°à¹„à¸£à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸µà¹‰",
            "à¸„à¸¸à¸“à¸Šà¹ˆà¸§à¸¢à¸Šà¸µà¹‰à¹à¸ˆà¸‡à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¹„à¸”à¹‰à¹„à¸«à¸¡",
            "à¸à¸£à¸¸à¸“à¸²à¹à¸ˆà¹‰à¸‡à¸Šà¸±à¸”à¹€à¸ˆà¸™",
            "à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸•à¸­à¸šà¹„à¸”à¹‰à¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™",
            "à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸š",
            "à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸šà¸­à¸à¹„à¸”à¹‰",
            "à¹„à¸¡à¹ˆà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸§à¹ˆà¸²à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸–à¸²à¸¡à¸­à¸°à¹„à¸£",
            "à¸£à¸°à¸šà¸¸à¸„à¸³à¸–à¸²à¸¡à¹ƒà¸«à¹‰à¸Šà¸±à¸”à¹€à¸ˆà¸™"
        ]
        return any(phrase in text for phrase in rejection_phrases)
    
    def check_greeting_message(text: str) -> bool:
        greeting_phrases = [
            "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š",
        ]
        return any(phrase in text for phrase in greeting_phrases)

    def ask(query, index, metadata, top_k=None, max_distance=None):
        global start
        start = time.perf_counter()
        top_k = len(query) if top_k is None else top_k
        if top_k > 10:
            top_k = 10

        if filter_user_query(query) < 0.4:
            end = time.perf_counter()
            processing_time = format_duration(end - start)
            return {
                "answer": "à¸‚à¸­à¸­à¸ à¸±à¸¢à¸„à¸£à¸±à¸š...à¸œà¸¡à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹„à¸”à¹‰....à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸œà¸¡à¸–à¸¹à¸à¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¸˜à¸£à¸£à¸¡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™",
                "duration": f'à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² {processing_time}'
            }
        else :
            results = search(query, index, metadata, top_k, max_distance)

        contexts = [r["doc"]["content"] for r in results]
        
        full_context = "\n".join(contexts)
        prompt = f"""à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡:\n{full_context}\nà¸„à¸³à¸–à¸²à¸¡: {query}"""
        model = 'gpt-oss:20b'
        log(f"Asking model: \"{model}\" with prompt:\n{prompt}")
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": "à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¸«à¸¥à¸²à¸¢à¹à¸«à¸¥à¹ˆà¸‡ à¸‹à¸¶à¹ˆà¸‡à¸ˆà¸°à¸Šà¹ˆà¸§à¸¢à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¸£à¸£à¸¡à¹à¸¥à¸°à¸ˆà¸°à¸•à¸­à¸šà¸”à¹‰à¸§à¸¢à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¹‚à¸”à¸¢à¹€à¸Šà¹‡à¸„à¸ˆà¸²à¸à¸„à¸³à¸–à¸²à¸¡à¸à¹ˆà¸­à¸™à¸‹à¸¶à¹ˆà¸‡à¸ˆà¸°à¸£à¸°à¸šà¸¸à¹ƒà¸™à¸šà¸£à¸£à¸—à¸±à¸”à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸–à¹‰à¸²à¹„à¸”à¹‰à¸£à¸±à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­à¸ˆà¸²à¸à¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¸£à¸£à¸¡à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸§à¹ˆà¸² à¸‚à¸­à¸­à¸ à¸±à¸¢à¸„à¸£à¸±à¸š...à¸œà¸¡à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹„à¸”à¹‰....à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸œà¸¡à¸–à¸¹à¸à¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¸˜à¸£à¸£à¸¡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™"},
                {"role": "user", "content": prompt}
            ]
        )
        answer = response['message']['content']
        if filter_user_query(answer) < 0.7:
            end = time.perf_counter()
            processing_time = format_duration(end - start)
            return {
                "answer": "à¸‚à¸­à¸­à¸ à¸±à¸¢à¸„à¸£à¸±à¸š...à¸œà¸¡à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸™à¸µà¹‰à¹„à¸”à¹‰....à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸œà¸¡à¸–à¸¹à¸à¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¸˜à¸£à¸£à¸¡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™",
                "duration": f'à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² {processing_time}'
            }
        ref_text = short_references([r["doc"] for r in results])
        end = time.perf_counter()
        processing_time = format_duration(end - start)
        log(f"Asked \"{model}\" finished in {processing_time}")
        if check_rejection_message(answer) or check_greeting_message(answer):
            return {
                "answer": answer,
                "duration": f'à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² {processing_time}'
            }
        else:
            return {
                "answer": answer,
                "references": f"à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸\n {ref_text}",
                "duration": f'à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² {processing_time}'
            }
    
    def read_last_embed_time():
        if not os.path.exists(STATUS_FILE):
            return "1970-01-01 00:00:00"
        with open(STATUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)["last_embed_time"]

    def init_bot():
        log("Checking for data updates...")
        embeddings, metadata = ensure_embeddings_up_to_date()
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        return index, metadata

    def parse_args(argv):
        message = None
        top_k = None
        max_distance = None

        i = 0
        while i < len(argv):
            arg = argv[i]
            if arg == '-k' and i + 1 < len(argv):
                try:
                    top_k = int(argv[i+1])
                except ValueError:
                    top_k = None
                i += 2
            elif arg == '-d' and i + 1 < len(argv):
                try:
                    max_distance = float(argv[i+1])
                except ValueError:
                    max_distance = None
                i += 2
            elif not arg.startswith('-') and message is None:
                message = arg
                i += 1
            else:
                i += 1

        return message, top_k, max_distance


    def ask_cli(argv=None):
        try:
            log(f"Starting BuddhamAI with argv: {argv}")
            if debug_mode == "false":
                check_and_pull_models(required_models)

            if argv is None:  # if not provided â†’ use sys.argv
                argv = sys.argv[1:]
                message, top_k, max_distance = parse_args(argv)
                
            log(f"Parsed arguments - message: {message}, top_k: {top_k}, max_distance: {max_distance}")

            if message is None or message.strip() == "":
                    result = {"answer": "à¸à¸£à¸¸à¸“à¸²à¸–à¸²à¸¡à¸„à¸³à¸–à¸²à¸¡", "references": "à¹„à¸¡à¹ˆà¸¡à¸µ", "duration": format_duration(0)}
                    data = {"data": result}
                    json_str = json.dumps(data, ensure_ascii=False)
                    log(json_str)
                    print(json_str)
                    return data  # return to main.py

            index, metadata = init_bot()
            if debug_mode == "true":
                time.sleep(float(os.getenv("DEBUG_TIME")))
                result = {"answer": "**à¸˜à¸£à¸£à¸¡ (Dhamma) à¸„à¸·à¸­ à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¹€à¸ˆà¹‰à¸²à¸ªà¸­à¸™à¹€à¸žà¸·à¹ˆà¸­à¸žà¹‰à¸™à¸—à¸¸à¸à¸‚à¹Œ**  - **à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¹‚à¸”à¸¢à¸£à¸§à¸¡**: â€œà¸˜à¸£à¸£à¸¡â€ à¸«à¸¡à¸²à¸¢à¸–à¸¶à¸‡ à¸à¸Žà¹à¸¥à¸°à¸«à¸¥à¸±à¸à¸˜à¸£à¸£à¸¡à¸—à¸µà¹ˆà¸žà¸£à¸°à¸žà¸¸à¸—à¸˜à¹€à¸ˆà¹‰à¸²à¸œà¸¹à¸à¹ƒà¸«à¹‰à¸ªà¸­à¸™à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡à¸ªà¸¹à¹ˆà¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸ˆà¸£à¸´à¸‡à¸‚à¸­à¸‡à¸Šà¸µà¸§à¸´à¸•à¹à¸¥à¸°à¸à¸²à¸£à¸”à¸±à¸šà¸—à¸¸à¸à¸‚à¹Œ  - **à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œà¸à¸±à¸šà¸­à¸£à¸´à¸¢à¸ªà¸±à¸ˆ 4**:    1. **à¸—à¸¸à¸à¸‚à¹Œ** â€“ à¸„à¸§à¸²à¸¡à¸—à¸¸à¸à¸‚à¹Œà¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¹„à¸¡à¹ˆà¸ªà¸šà¸²à¸¢à¹ƒà¸ˆà¹ƒà¸™à¸Šà¸µà¸§à¸´à¸•    2. **à¸ªà¸¡à¸¸à¸—à¸±à¸¢** â€“ à¹€à¸«à¸•à¸¸à¸‚à¸­à¸‡à¸—à¸¸à¸à¸‚à¹Œ (à¸„à¸§à¸²à¸¡à¸•à¸±à¸“à¸«à¸², à¸„à¸§à¸²à¸¡à¸­à¸¢à¸²à¸)    3. **à¸™à¸´à¹‚à¸£à¸˜** â€“ à¸à¸²à¸£à¸”à¸±à¸šà¸—à¸¸à¸à¸‚à¹Œ (à¸ªà¸±à¸™à¸•à¸´à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸—à¸¸à¸à¸‚à¹Œ)    4. **à¸¡à¸£à¸£à¸„** â€“ à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡ (à¸­à¸£à¸´à¸¢à¸­à¸¸à¸›à¸ªà¸£à¸£à¸„) à¸—à¸µà¹ˆà¸™à¸³à¹„à¸›à¸ªà¸¹à¹ˆà¸à¸²à¸£à¸”à¸±à¸šà¸—à¸¸à¸à¸‚à¹Œ  - **à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸Šà¸µà¸§à¸´à¸•à¸•à¸²à¸¡à¸˜à¸£à¸£à¸¡**:    * à¸à¸¶à¸à¸ªà¸•à¸´à¹à¸¥à¸°à¸›à¸à¸´à¸šà¸±à¸•à¸´à¹ƒà¸™à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™    * à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹à¸¥à¸°à¸—à¸³à¸•à¸²à¸¡à¸¡à¸£à¸£à¸„ (à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡ 8 à¸ªà¹ˆà¸§à¸™)    * à¸¥à¸”à¸•à¸±à¸“à¸«à¸²à¹à¸¥à¸°à¹à¸ªà¸§à¸‡à¸«à¸²à¸›à¸±à¸à¸à¸²à¸ˆà¸²à¸à¸„à¸§à¸²à¸¡à¸ˆà¸£à¸´à¸‡à¸‚à¸­à¸‡à¸˜à¸£à¸£à¸¡  à¸ªà¸£à¸¸à¸›à¹„à¸”à¹‰à¸§à¹ˆà¸² â€œà¸˜à¸£à¸£à¸¡â€ à¸„à¸·à¸­à¸«à¸¥à¸±à¸à¸à¸²à¸£à¹à¸¥à¸°à¸à¸Žà¸—à¸µà¹ˆà¸ªà¸­à¸™à¹ƒà¸«à¹‰à¸¡à¸™à¸¸à¸©à¸¢à¹Œà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸ˆà¸£à¸´à¸‡à¸‚à¸­à¸‡à¸Šà¸µà¸§à¸´à¸•à¹à¸¥à¸°à¸›à¸à¸›à¹‰à¸­à¸‡à¸•à¸™à¹€à¸­à¸‡à¸ˆà¸²à¸à¸„à¸§à¸²à¸¡à¸—à¸¸à¸à¸‚à¹Œ à¹‚à¸”à¸¢à¸à¸²à¸£à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸•à¸²à¸¡à¸­à¸£à¸´à¸¢à¸ªà¸±à¸ˆ 4 à¹à¸¥à¸°à¸¡à¸£à¸£à¸„.", "argv": message, "references": "test only", "duration": format_duration(0)} # for test only
                data = {"data": result}
                json_str = json.dumps(data, ensure_ascii=False)
                log(json_str)
                print(json_str)
                return data  # return to main.py
            
            result = ask(message, index, metadata, top_k=top_k, max_distance=max_distance)

            data = {"data": result}
            json_str = json.dumps(data, ensure_ascii=False)
            log(json_str)
            print(json_str)
            return data  # return to main.py
        except Exception:
            err_msg = traceback.format_exc()
            log("Error: " + err_msg)
            result = {"Error": f"Error: {err_msg}", "status": 500}
            data = {"data": result}
            json_str = json.dumps(data, ensure_ascii=False)
            log(json_str)
            print(json_str)
            return data  # return to main.py

    if __name__ == "__main__":
        ask_cli()
        
except Exception:
    err_msg = traceback.format_exc()
    try:
        log("Error: " + err_msg)
        result = {"Error": f"Error: {err_msg}", "status": 500}
        data = {"data": result}
        json_str = json.dumps(data, ensure_ascii=False)
        log(json_str)
        print(json_str)
    except:
        log("Error: " + err_msg)
        result = {"Error": f"Error: {err_msg}", "status": 500}
        data = {"data": result}
        json_str = json.dumps(data, ensure_ascii=False)
        log(json_str)
        print(json_str)
    sys.exit(1)